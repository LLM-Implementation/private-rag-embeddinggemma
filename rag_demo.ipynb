{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Private RAG Stack with EmbeddingGemma & SQLite-vec\n",
    "\n",
    "## üîí 100% Private | üí∞ Zero Cost | üì± Offline Capable\n",
    "\n",
    "This notebook demonstrates how to build a complete RAG (Retrieval Augmented Generation) system using:\n",
    "\n",
    "- **EmbeddingGemma**: Google's efficient 300M parameter embedding model\n",
    "- **SQLite-vec**: Fast vector similarity search in SQLite\n",
    "- **Qwen3**: Efficient local language model via Ollama\n",
    "\n",
    "### What You'll Learn:\n",
    "1. How to scrape and prepare documentation\n",
    "2. Generate embeddings with EmbeddingGemma\n",
    "3. Store vectors in SQLite with sqlite-vec\n",
    "4. Perform semantic search\n",
    "5. Generate contextual responses with local LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Step 1: Import Required Libraries\n",
    "\n",
    "We'll use a minimal set of libraries for our RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import sqlite_vec\n",
    "import ollama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import struct\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 2: Configuration & Helper Functions\n",
    "\n",
    "Let's set up our configuration and utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "EMBEDDING_MODEL = 'google/embeddinggemma-300m'  # Google's new EmbeddingGemma model\n",
    "EMBEDDING_DIMS = 256  # Truncated from 768 for 3x faster processing\n",
    "LLM_MODEL = 'qwen3:4b'  # Efficient 4B parameter model via Ollama\n",
    "DB_FILE = \"rag_vectors.db\"\n",
    "TABLE_NAME = \"documents\"\n",
    "\n",
    "# Vector serialization helper\n",
    "def serialize_f32(vector):\n",
    "    \"\"\"Convert float vector to bytes for SQLite storage\"\"\"\n",
    "    return struct.pack(\"%sf\" % len(vector), *vector)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Step 3: Scrape Documentation\n",
    "\n",
    "We'll scrape official documentation from key sources to build our knowledge base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Scraping documentation...\n",
      "üìÑ Fetching: sqlite_vec_python\n",
      "   ‚úÖ Saved: sqlite_vec_python (4801 chars)\n",
      "üìÑ Fetching: sqlite_vec_demo\n",
      "   ‚úÖ Saved: sqlite_vec_demo (1213 chars)\n",
      "üìÑ Fetching: embeddinggemma_google_blog\n",
      "   ‚úÖ Saved: embeddinggemma_google_blog (9745 chars)\n",
      "üìÑ Fetching: huggingface_embeddinggemma\n",
      "   ‚úÖ Saved: huggingface_embeddinggemma (13384 chars)\n",
      "üìÑ Fetching: huggingface_embeddinggemma_blog\n",
      "   ‚úÖ Saved: huggingface_embeddinggemma_blog (47633 chars)\n",
      "üìÑ Fetching: qwen3_ollama\n",
      "   ‚úÖ Saved: qwen3_ollama (4949 chars)\n",
      "üìÑ Fetching: sentence_transformers\n",
      "   ‚úÖ Saved: sentence_transformers (79545 chars)\n",
      "\n",
      "üìä Successfully scraped 7 documents\n"
     ]
    }
   ],
   "source": [
    "# Documentation sources for our RAG system\n",
    "docs_to_scrape = {\n",
    "    'sqlite_vec_python': 'https://alexgarcia.xyz/sqlite-vec/python.html',\n",
    "    'sqlite_vec_demo': 'https://raw.githubusercontent.com/asg017/sqlite-vec/main/examples/simple-python/demo.py',\n",
    "    'embeddinggemma_google_blog': 'https://developers.googleblog.com/en/introducing-embeddinggemma/',\n",
    "    'huggingface_embeddinggemma': 'https://huggingface.co/google/embeddinggemma-300m',\n",
    "    'huggingface_embeddinggemma_blog': 'https://huggingface.co/blog/embeddinggemma',\n",
    "    'qwen3_ollama': 'https://ollama.com/library/qwen3',\n",
    "    'sentence_transformers': 'https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'\n",
    "}\n",
    "\n",
    "# Create docs directory\n",
    "os.makedirs('docs', exist_ok=True)\n",
    "\n",
    "print(\"üì• Scraping documentation...\")\n",
    "scraped_docs = []\n",
    "\n",
    "for name, url in docs_to_scrape.items():\n",
    "    try:\n",
    "        print(f\"üìÑ Fetching: {name}\")\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse and extract text\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for element in soup(['script', 'style']):\n",
    "            element.decompose()\n",
    "        \n",
    "        text_content = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        if text_content.strip():\n",
    "            # Save locally\n",
    "            filepath = f\"docs/{name}.txt\"\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"Source: {url}\\n\")\n",
    "                f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "                f.write(text_content)\n",
    "            \n",
    "            scraped_docs.append((name, text_content))\n",
    "            print(f\"   ‚úÖ Saved: {name} ({len(text_content)} chars)\")\n",
    "        \n",
    "        time.sleep(1)  # Be respectful to servers\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error fetching {name}: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Successfully scraped {len(scraped_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Step 4: Initialize EmbeddingGemma Model\n",
    "\n",
    "Load Google's EmbeddingGemma for generating high-quality embeddings:\n",
    "\n",
    "> **Note**: You need to request access at https://huggingface.co/google/embeddinggemma-300m and run `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading google/embeddinggemma-300m...\n",
      "‚úÖ EmbeddingGemma loaded successfully!\n",
      "Sample embedding: [-1.27548069e-01 -1.03747128e-02  6.00063242e-03  3.82866375e-02\n",
      "  3.39595117e-02  7.22874403e-02 -3.14668380e-03  6.48741052e-02\n",
      "  3.73455212e-02 -3.45372595e-02 -2.08048206e-02  1.68610997e-02\n",
      " -1.10374615e-02 -3.13950144e-02  9.01036486e-02 -1.35720125e-03\n",
      " -3.91047597e-02  8.65481852e-04 -3.12702060e-02  2.52648396e-03\n",
      "  3.65278162e-02  2.71117315e-02  3.11176339e-03 -1.19482335e-02\n",
      " -2.34869476e-02  2.15606447e-02 -1.87460158e-03 -7.75819346e-02\n",
      " -2.47754939e-02  1.50251044e-02  7.44022150e-03 -1.15102921e-02\n",
      "  7.88129028e-03 -6.16652519e-03  2.06527431e-02 -3.50111071e-03\n",
      " -3.06493905e-03 -4.67991307e-02  3.31943296e-02  3.31700258e-02\n",
      " -2.72768941e-02  5.07247038e-02 -3.14996615e-02  2.65736580e-02\n",
      "  3.35765183e-02 -4.00158912e-02 -5.88491410e-02 -2.59615704e-02\n",
      "  2.05058302e-03 -2.49158852e-02  6.53840415e-03  4.06379402e-02\n",
      "  8.83991644e-03 -1.37844970e-02 -3.76720503e-02  9.19052958e-03\n",
      "  6.11931086e-04 -1.45705314e-02 -1.90025065e-02 -1.36232208e-02\n",
      " -8.49440992e-02 -1.97249260e-02 -4.31350544e-02 -6.16135262e-03\n",
      "  6.20216317e-02 -4.70677465e-02  3.81164961e-02  2.54458021e-02\n",
      " -2.84389849e-03  2.69870698e-01 -2.56258976e-02 -3.17546651e-02\n",
      " -2.99419020e-03 -8.59707221e-02  2.07825005e-01  6.69404715e-02\n",
      " -6.70872908e-03 -5.07267900e-02 -3.53356898e-02  3.09303938e-03\n",
      " -2.64340322e-02  3.01629957e-02  4.60091904e-02 -2.93641370e-02\n",
      "  1.08281992e-01  1.05226794e-02 -8.74237623e-03 -5.83168194e-02\n",
      "  4.21760976e-02 -3.36132087e-02 -2.55027469e-02 -9.39440855e-04\n",
      " -3.56784128e-02  9.29109938e-03  1.80632586e-03 -1.28998868e-02\n",
      " -8.91005807e-03  6.18194276e-03 -2.47800071e-03 -3.32213216e-03\n",
      " -5.40955598e-03  1.38415094e-03  4.62654196e-02  1.36231631e-01\n",
      "  2.94852424e-02 -4.24910933e-02 -1.08460672e-02  1.61583759e-02\n",
      "  1.31301163e-02  4.72245105e-02 -1.64793227e-02  4.00536368e-03\n",
      " -3.04749757e-02 -9.41208098e-03  2.23995093e-03 -3.10638268e-02\n",
      " -2.82595865e-02  7.61594772e-02 -2.47002253e-03 -2.43779756e-02\n",
      "  5.09212948e-02  2.24584918e-02  6.72571536e-04 -8.39676242e-03\n",
      "  2.66481303e-02  6.09997734e-02 -1.11896759e-02  3.48457019e-03\n",
      " -4.27659415e-02  3.84671707e-03  1.05915712e-02 -1.52510665e-02\n",
      "  6.65869713e-02 -4.63590171e-04  6.89128274e-03  6.19322211e-02\n",
      " -2.45829411e-02  1.62489656e-02  1.19988807e-01  2.71373019e-02\n",
      " -1.86183467e-03 -2.34222356e-02 -2.57751402e-02 -5.70988981e-03\n",
      " -1.53391091e-02  5.62961139e-02 -7.04612806e-02 -4.34827954e-02\n",
      " -3.20494883e-02 -1.67828444e-02  1.21719465e-02  4.85384986e-02\n",
      "  1.90733057e-02 -2.33176332e-02 -9.96317901e-03  2.46522878e-03\n",
      "  1.35384779e-02 -4.64508450e-03 -1.71493702e-02  1.10326912e-02\n",
      " -2.11240929e-02 -3.28514837e-02  1.68255940e-02  5.53547107e-02\n",
      "  2.61198226e-02  8.32986236e-02  1.97378546e-02 -4.81248222e-04\n",
      "  1.67482693e-04  4.23316769e-02 -6.30504712e-02 -5.66522442e-02\n",
      " -6.30508224e-03 -3.14505445e-03 -3.08780801e-02  4.54236902e-02\n",
      " -8.17211904e-03  3.84354293e-02  3.07392403e-02 -6.00691512e-03\n",
      " -3.80377583e-02  2.53287107e-02 -6.93449080e-02  1.36612086e-02\n",
      " -2.08899640e-02  1.93450507e-02 -2.83375960e-02  5.39382584e-02\n",
      " -2.14518085e-02 -4.39440943e-02  8.11488181e-03  2.94358805e-02\n",
      " -1.64834671e-02 -6.04470260e-02  5.67340180e-02  1.93626974e-02\n",
      "  1.23654149e-01  1.20976651e-02 -4.39098962e-02 -2.54113823e-02\n",
      "  1.34310164e-02 -1.18063753e-02 -1.19570121e-02  6.80421619e-03\n",
      " -1.19133098e-02  6.55462313e-03 -2.07023565e-02  2.73005441e-02\n",
      " -3.26051377e-02  2.96629928e-02 -3.34867872e-02  2.83586746e-03\n",
      " -9.55387950e-03 -5.84019907e-02  1.18852826e-02  1.77858453e-02\n",
      " -3.15076811e-03  1.71895884e-02  1.14509715e-02 -2.23007463e-02\n",
      " -1.16603347e-02  1.98357739e-02 -4.16652672e-02 -8.69523361e-03\n",
      " -4.72454689e-02  1.72562506e-02 -2.95116790e-02 -6.83706775e-02\n",
      "  6.69111777e-03  7.67063582e-03  1.90441180e-02  2.05587447e-02\n",
      "  3.48697677e-02  7.22078308e-02 -6.80504218e-02 -1.23179806e-02\n",
      " -1.54595375e-02 -1.28591142e-03  7.55512714e-03 -4.92220977e-03\n",
      " -1.51233014e-03 -3.68854566e-03 -2.53909919e-02  1.31221516e-02\n",
      "  1.41145810e-02 -1.39027592e-02  1.96975004e-02 -3.45240813e-03\n",
      "  2.69101206e-02 -3.91725637e-02 -7.40243681e-03  5.63159175e-02\n",
      "  7.31503963e-02 -1.34890778e-02  2.33444180e-02  1.50543163e-02]\n",
      "üìä Sample embedding shape: (256,)\n",
      "üí° Using 256 dimensions for 3x faster processing\n"
     ]
    }
   ],
   "source": [
    "# Load EmbeddingGemma model\n",
    "print(f\"ü§ñ Loading {EMBEDDING_MODEL}...\")\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(\"‚úÖ EmbeddingGemma loaded successfully!\")\n",
    "\n",
    "# Test the model with a sample\n",
    "sample_text = \"EmbeddingGemma is Google's efficient embedding model\"\n",
    "sample_embedding = embedding_model.encode_document(sample_text, truncate_dim=EMBEDDING_DIMS)\n",
    "print(f\"Sample embedding: {sample_embedding}\")\n",
    "print(f\"üìä Sample embedding shape: {sample_embedding.shape}\")\n",
    "print(f\"üí° Using {EMBEDDING_DIMS} dimensions for 3x faster processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Step 5: Setup SQLite Vector Database\n",
    "\n",
    "Initialize SQLite with the sqlite-vec extension for efficient vector storage and similarity search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÑÔ∏è Setting up vector database...\n",
      "‚úÖ Vector database ready!\n"
     ]
    }
   ],
   "source": [
    "# Initialize SQLite with vector extension\n",
    "print(\"üóÑÔ∏è Setting up vector database...\")\n",
    "\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "conn.enable_load_extension(True)\n",
    "sqlite_vec.load(conn)\n",
    "conn.enable_load_extension(False)\n",
    "\n",
    "# Create vector table\n",
    "conn.execute(f\"\"\"\n",
    "    CREATE VIRTUAL TABLE IF NOT EXISTS {TABLE_NAME} USING vec0(\n",
    "        text TEXT,\n",
    "        source TEXT,\n",
    "        embedding float[{EMBEDDING_DIMS}]\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "print(\"‚úÖ Vector database ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 6: Smart Token-Based Chunking & Embeddings\n",
    "\n",
    "Process our scraped documents using intelligent token-based chunking for optimal embedding quality:\n",
    "\n",
    "**Why Token-Based Chunking?**\n",
    "- Uses the **same tokenizer** as EmbeddingGemma for perfect alignment\n",
    "- **Respects token boundaries** instead of arbitrary character limits\n",
    "- **Prevents word splitting** that degrades embedding quality\n",
    "- **Consistent chunk sizes** measured in actual tokens, not characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Chunking documents with token-based precision...\n",
      "üìÑ sqlite_vec_python: 1 chunks\n",
      "üìÑ sqlite_vec_demo: 1 chunks\n",
      "üìÑ embeddinggemma_google_blog: 2 chunks\n",
      "üìÑ huggingface_embeddinggemma: 2 chunks\n",
      "üìÑ huggingface_embeddinggemma_blog: 8 chunks\n",
      "üìÑ qwen3_ollama: 1 chunks\n",
      "üìÑ sentence_transformers: 13 chunks\n",
      "\n",
      "üìä Total chunks created: 28\n",
      "üßÆ Total embeddings generated: 28\n",
      "üí° Using token-based chunking ensures optimal embedding quality!\n"
     ]
    }
   ],
   "source": [
    "def token_based_chunking(text, tokenizer, max_tokens=2048, overlap_tokens=100):\n",
    "    \"\"\"\n",
    "    Token-based chunking using the actual embedding model's tokenizer.\n",
    "    Much more accurate than character-based chunking for optimal embeddings.\n",
    "    \"\"\"\n",
    "    # Tokenize the entire text\n",
    "    tokens = tokenizer.encode(text)\n",
    "    \n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [text]  # No need to chunk\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(tokens):\n",
    "        # Get chunk tokens\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        \n",
    "        # Decode back to text\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text.strip())\n",
    "        \n",
    "        # Move start position with overlap\n",
    "        if end >= len(tokens):\n",
    "            break\n",
    "        start = end - overlap_tokens\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def chunk_text(text, model, max_tokens=2048, overlap_tokens=100):\n",
    "    \"\"\"Use token-based chunking with the embedding model's tokenizer.\"\"\"\n",
    "    return token_based_chunking(text, model.tokenizer, max_tokens, overlap_tokens)\n",
    "\n",
    "# Process all documents with intelligent token-based chunking\n",
    "print(\"üìù Chunking documents with token-based precision...\")\n",
    "all_chunks = []\n",
    "all_sources = []\n",
    "all_embeddings = []\n",
    "\n",
    "for source_name, content in scraped_docs:\n",
    "    # Use token-based chunking with the embedding model's tokenizer\n",
    "    chunks = chunk_text(content, embedding_model, max_tokens=2048, overlap_tokens=100)\n",
    "    print(f\"üìÑ {source_name}: {len(chunks)} chunks\")\n",
    "    \n",
    "    # Generate embeddings for all chunks\n",
    "    chunk_embeddings = []\n",
    "    for chunk in chunks:\n",
    "        # Use document encoding with proper prompt\n",
    "        embedding = embedding_model.encode_document(chunk, truncate_dim=EMBEDDING_DIMS)\n",
    "        chunk_embeddings.append(embedding)\n",
    "    \n",
    "    all_chunks.extend(chunks)\n",
    "    all_sources.extend([source_name] * len(chunks))\n",
    "    all_embeddings.extend(chunk_embeddings)\n",
    "\n",
    "print(f\"\\nüìä Total chunks created: {len(all_chunks)}\")\n",
    "print(f\"üßÆ Total embeddings generated: {len(all_embeddings)}\")\n",
    "print(f\"üí° Using token-based chunking ensures optimal embedding quality!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Store Embeddings in Vector Database\n",
    "\n",
    "Insert all our document chunks and their embeddings into the SQLite vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Storing embeddings in vector database...\n",
      "üîÑ Processed 10/28 chunks...\n",
      "üîÑ Processed 20/28 chunks...\n",
      "\n",
      "‚úÖ All embeddings stored successfully!\n",
      "üìä Database contains 28 documents ready for search\n"
     ]
    }
   ],
   "source": [
    "# Store all chunks and embeddings\n",
    "print(\"üíæ Storing embeddings in vector database...\")\n",
    "\n",
    "for i, (chunk, source, embedding) in enumerate(zip(all_chunks, all_sources, all_embeddings)):\n",
    "    conn.execute(f\"\"\"\n",
    "        INSERT INTO {TABLE_NAME} (rowid, text, source, embedding)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "    \"\"\", (i + 1, chunk, source, serialize_f32(embedding.tolist())))\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"üîÑ Processed {i + 1}/{len(all_chunks)} chunks...\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"\\n‚úÖ All embeddings stored successfully!\")\n",
    "\n",
    "# Verify our data\n",
    "cursor = conn.execute(f\"SELECT COUNT(*) FROM {TABLE_NAME}\")\n",
    "count = cursor.fetchone()[0]\n",
    "print(f\"üìä Database contains {count} documents ready for search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 8: Semantic Search Function\n",
    "\n",
    "Create a function to perform semantic search using our vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing search with query: 'How does EmbeddingGemma work?'\n",
      "üîç Found 3 relevant documents:\n",
      "üìÑ Source: huggingface_embeddinggemma_blog | Distance: 0.5917\n",
      "üìù Preview: <bos>Welcome EmbeddingGemma, Google's new efficient embedding model\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spa...\n",
      "\n",
      "üìÑ Source: embeddinggemma_google_blog | Distance: 0.6111\n",
      "üìù Preview: <bos>Introducing EmbeddingGemma: The Best-in-Class Open Model for On-Device Embeddings\n",
      "            \n",
      "...\n",
      "\n",
      "üìÑ Source: huggingface_embeddinggemma | Distance: 0.6134\n",
      "üìù Preview: .70\n",
      "68.70\n",
      "Mixed Precision* (768d)\n",
      "68.03\n",
      "68.03\n",
      "Note: QAT models are evaluated after quantization\n",
      "* Mi...\n",
      "\n",
      "‚úÖ Search test completed!\n"
     ]
    }
   ],
   "source": [
    "def semantic_search(query_text, top_k=3):\n",
    "    \"\"\"Perform semantic search and return relevant documents\"\"\"\n",
    "    \n",
    "    # Generate query embedding using proper query prompt\n",
    "    query_embedding = embedding_model.encode_query(query_text, truncate_dim=EMBEDDING_DIMS)\n",
    "    \n",
    "    # Search for similar documents\n",
    "    cursor = conn.execute(f\"\"\"\n",
    "        SELECT rowid, text, source, distance\n",
    "        FROM {TABLE_NAME}\n",
    "        WHERE embedding MATCH ?\n",
    "        ORDER BY distance\n",
    "        LIMIT ?\n",
    "    \"\"\", (serialize_f32(query_embedding.tolist()), top_k))\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    print(f\"üîç Found {len(results)} relevant documents:\")\n",
    "    contexts = []\n",
    "    \n",
    "    for rowid, text, source, distance in results:\n",
    "        contexts.append(text)\n",
    "        print(f\"üìÑ Source: {source} | Distance: {distance:.4f}\")\n",
    "        print(f\"üìù Preview: {text[:100]}...\\n\")\n",
    "    \n",
    "    return contexts\n",
    "\n",
    "# Test semantic search\n",
    "test_query = \"How does EmbeddingGemma work?\"\n",
    "print(f\"üß™ Testing search with query: '{test_query}'\")\n",
    "test_results = semantic_search(test_query)\n",
    "print(f\"‚úÖ Search test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 9: RAG Query Function with Local LLM\n",
    "\n",
    "Combine semantic search with local LLM to generate contextual responses:\n",
    "\n",
    "> **Note**: Make sure you have Ollama installed and the Qwen3 model downloaded: `ollama pull qwen3:4b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG query function ready!\n"
     ]
    }
   ],
   "source": [
    "def rag_query(question, top_k=3):\n",
    "    \"\"\"Complete RAG pipeline: search + generate response\"\"\"\n",
    "    \n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Semantic search\n",
    "    contexts = semantic_search(question, top_k)\n",
    "    \n",
    "    if not contexts:\n",
    "        return \"‚ùå No relevant information found.\"\n",
    "    \n",
    "    # Step 2: Build prompt with context\n",
    "    combined_context = \"\\n\\n\".join(contexts)\n",
    "    prompt = f\"\"\"Use the following contexts to answer the question comprehensively.\n",
    "If you don't know the answer based on the provided contexts, just say that you don't know.\n",
    "\n",
    "Contexts:\n",
    "{combined_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Step 3: Generate response with local LLM\n",
    "    print(f\"ü§ñ Generating response with {LLM_MODEL}...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Stream response for real-time output\n",
    "        stream = ollama.chat(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        response = \"\"\n",
    "        for chunk in stream:\n",
    "            if 'message' in chunk and 'content' in chunk['message']:\n",
    "                content = chunk['message']['content']\n",
    "                print(content, end='', flush=True)\n",
    "                response += content\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error with LLM: {e}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "print(\"‚úÖ RAG query function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 10: Demo Queries - Let's Test Our RAG System!\n",
    "\n",
    "Now let's put our RAG system to work with some interesting questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: What makes EmbeddingGemma special for mobile applications?\n",
      "============================================================\n",
      "üîç Found 3 relevant documents:\n",
      "üìÑ Source: embeddinggemma_google_blog | Distance: 0.5936\n",
      "üìù Preview: <bos>Introducing EmbeddingGemma: The Best-in-Class Open Model for On-Device Embeddings\n",
      "            \n",
      "...\n",
      "\n",
      "üìÑ Source: huggingface_embeddinggemma_blog | Distance: 0.6205\n",
      "üìù Preview: <bos>Welcome EmbeddingGemma, Google's new efficient embedding model\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spa...\n",
      "\n",
      "üìÑ Source: huggingface_embeddinggemma | Distance: 0.6275\n",
      "üìù Preview: .70\n",
      "68.70\n",
      "Mixed Precision* (768d)\n",
      "68.03\n",
      "68.03\n",
      "Note: QAT models are evaluated after quantization\n",
      "* Mi...\n",
      "\n",
      "ü§ñ Generating response with qwen3:4b...\n",
      "\n",
      "<think>\n",
      "Let me analyze the text to understand what makes EmbeddingGemma special for mobile applications.\n",
      "\n",
      "From the provided text, I can find several key points about EmbeddingGemma and its mobile applications:\n",
      "\n",
      "1. The text mentions \"Use in mobile applications\" in the context of the model's capabilities.\n",
      "\n",
      "2. There's a section about \"Mobile Applications\" that states:\n",
      "\"EmbeddingGemma is designed to be used in mobile applications with low latency and high accuracy. It supports both iOS and Android platforms.\"\n",
      "\n",
      "3. The text also mentions that \"EmbeddingGemma is optimized for mobile devices with low memory footprint and efficient inference.\"\n",
      "\n",
      "4. Another relevant section states: \"EmbeddingG: A lightweight model that can run on mobile devices with minimal latency\"\n",
      "\n",
      "5. The text also mentions \"Use in mobile applications\" as a key use case.\n",
      "\n",
      "Let me also check if there's any specific information about mobile optimization in the model's description:\n",
      "\n",
      "The text mentions that EmbeddingGemma is a \"lightweight model\" that can run on mobile devices with \"minimal latency\". It's specifically designed for mobile applications with \"low memory footprint\" and \"efficient inference\".\n",
      "\n",
      "Additionally, I see that there's a section about \"Mobile Applications\" that says:\n",
      "\"EmbeddingGemma is designed to be used in mobile applications with low latency and high accuracy. It supports both iOS and Android platforms.\"\n",
      "\n",
      "So, the special features for mobile applications seem to be:\n",
      "1. Low latency\n",
      "2. High accuracy\n",
      "3. Low memory footprint\n",
      "4. Efficient inference\n",
      "5. Support for both iOS and Android platforms\n",
      "\n",
      "The text also mentions that it's part of a family of models that are \"designed from the ground up for responsible AI development\".\n",
      "\n",
      "Let me check if there's any more specific information about the mobile optimization in the text.\n",
      "\n",
      "I see that the model is described as \"a lightweight model that can run on mobile devices with minimal latency\". This suggests that it's optimized for mobile hardware constraints.\n",
      "\n",
      "Also, the text mentions that it can be used with \"Text Embeddings Inference (TEI)\" for efficient deployment on various hardware configurations.\n",
      "\n",
      "For the answer, I should focus on the specific mobile application benefits mentioned in the text.\n",
      "\n",
      "Based on the text, EmbeddingGemma is special for mobile applications because:\n",
      "- It has low latency\n",
      "- It has high accuracy\n",
      "- It has a low memory footprint\n",
      "- It supports both iOS and Android platforms\n",
      "- It's optimized for efficient inference on mobile devices\n",
      "\n",
      "Let me formulate a concise answer based on this information.\n",
      "</think>\n",
      "\n",
      "Based on the provided text, EmbeddingGemma is special for mobile applications because:\n",
      "\n",
      "1. It's designed specifically for mobile use with **low latency** and **high accuracy**\n",
      "2. It has a **low memory footprint** that works well on mobile devices\n",
      "3. It provides **efficient inference** optimized for mobile hardware constraints\n",
      "4. It supports both **iOS and Android platforms**\n",
      "5. It's part of a family of models that \"are designed from the ground up for responsible AI development\" with minimal resource requirements\n",
      "\n",
      "The text specifically mentions that EmbeddingGemma is a \"lightweight model that can run on mobile devices with minimal latency,\" making it particularly suitable for mobile applications where processing power and memory are limited.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Demo Question 1: About EmbeddingGemma\n",
    "response1 = rag_query(\"What makes EmbeddingGemma special for mobile applications?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: How do I use SQLite-vec with Python?\n",
      "============================================================\n",
      "üîç Found 3 relevant documents:\n",
      "üìÑ Source: sqlite_vec_python | Distance: 0.5931\n",
      "üìù Preview: sqlite-vec in Python | sqlite-vec\n",
      "üößüößüöß This documentation is a work-in-progress! üößüößüöß\n",
      "Skip to content\n",
      "...\n",
      "\n",
      "üìÑ Source: sqlite_vec_demo | Distance: 0.6076\n",
      "üìù Preview: import sqlite3\n",
      "import sqlite_vec\n",
      "\n",
      "from typing import List\n",
      "import struct\n",
      "\n",
      "\n",
      "def serialize_f32(vector: ...\n",
      "\n",
      "üìÑ Source: sentence_transformers | Distance: 0.6535\n",
      "üìù Preview: start_multi_process_pool.\n",
      "Parameters\n",
      ":\n",
      "pool\n",
      "(\n",
      "Dict\n",
      "[\n",
      "str\n",
      ",\n",
      "object\n",
      "]\n",
      ") ‚Äì A dictionary containing the ...\n",
      "\n",
      "ü§ñ Generating response with qwen3:4b...\n",
      "\n",
      "<think>\n",
      "Let me analyze the provided contexts to answer the question about how to use SQLite-vec with Python.\n",
      "\n",
      "The contexts include documentation for sqlite-vec, specifically the Python section. Here's what I can extract:\n",
      "\n",
      "1. Installation: The documentation says to install the sqlite-vec PyPi package using pip:\n",
      "   ```bash\n",
      "   pip install sqlite-vec\n",
      "   ```\n",
      "\n",
      "2. Loading the extension: After installation, use the `sqlite_vec.load()` function to load sqlite-vec SQL functions into a SQLite connection:\n",
      "   ```python\n",
      "   import sqlite3\n",
      "   import sqlite_vec\n",
      "\n",
      "   db = sqlite3.connect(\":memory:\")\n",
      "   db.enable_load_extension(True)\n",
      "   sqlite_vec.load(db)\n",
      "   db.enable_load_extension(False)\n",
      "   ```\n",
      "\n",
      "3. Checking the version: After loading, you can check the version with:\n",
      "   ```python\n",
      "   vec_version, = db.execute(\"select vec_version()\").fetchone()\n",
      "   print(f\"vec_version={vec_version}\")\n",
      "   ```\n",
      "\n",
      "4. Working with vectors:\n",
      "   - For lists of floats: Use `serialize_float32()` to convert to the compact BLOB format\n",
      "   - For NumPy arrays: Can be passed directly as-is (after casting to 32-bit floats)\n",
      "\n",
      "5. Using an up-to-date SQLite: The documentation mentions that some features require SQLite 3.41 or higher.\n",
      "\n",
      "6. There's also an example Python script (simple-python/demo.py) mentioned as a more complete example.\n",
      "\n",
      "7. There's also a code snippet showing how to use sqlite-vec with Python, including:\n",
      "   - Creating a virtual table using vec0\n",
      "   - Inserting vectors\n",
      "   - Performing vector similarity searches\n",
      "\n",
      "So to answer the question \"How do I use SQLite-vec with Python?\", I'll provide a comprehensive answer based on the documentation.\n",
      "\n",
      "The answer should include:\n",
      "1. Installation steps\n",
      "2. Basic usage steps\n",
      "3. Working with different vector types (lists, NumPy arrays)\n",
      "4. Example of using with vector similarity search\n",
      "5. Note about requiring an up-to-date SQLite library\n",
      "\n",
      "Let me craft a comprehensive answer based on this information.\n",
      "</think>\n",
      "\n",
      "# How to Use SQLite-vec with Python\n",
      "\n",
      "Based on the provided documentation, here's a comprehensive guide on using SQLite-vec with Python:\n",
      "\n",
      "## 1. Installation\n",
      "\n",
      "First, install the sqlite-vec Python package using pip:\n",
      "\n",
      "```bash\n",
      "pip install sqlite-vec\n",
      "```\n",
      "\n",
      "## 2. Basic Setup\n",
      "\n",
      "After installation, load the sqlite-vec extension into your SQLite connection:\n",
      "\n",
      "```python\n",
      "import sqlite3\n",
      "import sqlite_vec\n",
      "\n",
      "# Create an in-memory database\n",
      "db = sqlite3.connect(\":memory:\")\n",
      "\n",
      "# Enable loading of SQLite extensions\n",
      "db.enable_load_extension(True)\n",
      "\n",
      "# Load sqlite-vec functions into the connection\n",
      "sqlite_vec.load(db)\n",
      "\n",
      "# Disable extension loading again (optional)\n",
      "db.enable_load_extension(False)\n",
      "```\n",
      "\n",
      "## 3. Check Version\n",
      "\n",
      "Verify the installed version of sqlite-vec:\n",
      "\n",
      "```python\n",
      "vec_version, = db.execute(\"select vec_version()\").fetchone()\n",
      "print(f\"vec_version={vec_version}\")\n",
      "```\n",
      "\n",
      "## 4. Working with Vectors\n",
      "\n",
      "SQLite-vec supports two main ways to work with vectors in Python:\n",
      "\n",
      "### A. Using Lists of Floats\n",
      "\n",
      "If your vectors are provided as Python lists of floats, convert them to the compact BLOB format using `serialize_float32()`:\n",
      "\n",
      "```python\n",
      "from sqlite_vec import serialize_float32\n",
      "\n",
      "embedding = [0.1, 0.2, 0.3, 0.4]\n",
      "serialized_embedding = serialize_float32(embedding)\n",
      "```\n",
      "\n",
      "### B. Using NumPy Arrays\n",
      "\n",
      "For NumPy arrays (after casting to 32-bit floats):\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "embedding = np.array([0.1, 0.2, 0.3, 0.4], dtype=np.float32)\n",
      "```\n",
      "\n",
      "## 5. Complete Example: Vector Similarity Search\n",
      "\n",
      "Here's a complete example of using sqlite-vec for vector similarity search:\n",
      "\n",
      "```python\n",
      "import sqlite3\n",
      "import sqlite_vec\n",
      "import numpy as np\n",
      "\n",
      "# Create database and table\n",
      "db = sqlite3.connect(\":memory:\")\n",
      "db.enable_load_extension(True)\n",
      "sqlite_vec.load(db)\n",
      "\n",
      "# Create a vector table\n",
      "cursor = db.cursor()\n",
      "cursor.execute(\"\"\"\n",
      "    CREATE TABLE IF NOT EXISTS embeddings (\n",
      "        id INTEGER PRIMARY KEY,\n",
      "        vector BLOB\n",
      "    )\n",
      "\"\"\")\n",
      "db.commit()\n",
      "\n",
      "# Insert some vectors\n",
      "vectors = [\n",
      "    np.array([0.1, 0.2, 0.3, 0.4], dtype=np.float32),\n",
      "    np.array([0.5, 0.6, 0.7, 0.8], dtype=np.float32)\n",
      "]\n",
      "for i, vec in enumerate(vectors):\n",
      "    cursor.execute(\"INSERT INTO embeddings (vector) VALUES (?)\", (sqlite_vec.serialize_float32(vec),))\n",
      "db.commit()\n",
      "\n",
      "# Perform a similarity search\n",
      "query = np.array([0.2, 0.3, 0.4, 0.5], dtype=np.float32)\n",
      "query_blob = sqlite_vec.serialize_float32(query)\n",
      "\n",
      "# Get top 1 match\n",
      "cursor.execute(\"\"\"\n",
      "    SELECT id, vector\n",
      "    FROM embeddings\n",
      "    ORDER BY cosine_similarity(?, vector) DESC\n",
      "    LIMIT 1\n",
      "\"\"\", (query_blob,))\n",
      "result = cursor.fetchone()\n",
      "\n",
      "print(f\"Closest match: {result[0]} with similarity score: {result[1]}\")\n",
      "```\n",
      "\n",
      "## 6. Important Notes\n",
      "\n",
      "- **SQLite Version**: Ensure you have SQLite 3.41 or newer (required for some features)\n",
      "- **Installation Requirements**: Make sure your Python environment has the required dependencies\n",
      "- **Complete Example**: The documentation mentions a more complete example in `simple-python/demo.py`\n",
      "- **Vector Operations**: For advanced vector operations, you'll need to use the `cosine_similarity` function provided by sqlite-vec\n",
      "\n",
      "This implementation allows you to efficiently store and query vector data using SQLite with Python, which is particularly useful for applications like semantic search, recommendation systems, and other AI applications that require vector similarity calculations.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Demo Question 2: About SQLite-vec\n",
    "response2 = rag_query(\"How do I use SQLite-vec with Python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: What are the key features of Qwen3 model?\n",
      "============================================================\n",
      "üîç Found 3 relevant documents:\n",
      "üìÑ Source: huggingface_embeddinggemma_blog | Distance: 0.6556\n",
      "üìù Preview: enumerate\n",
      "(eval_dataset[\n",
      "\"passage_text\"\n",
      "] + train_dataset[\n",
      "\"passage_text\"\n",
      "][:\n",
      "30_000\n",
      "]))\n",
      "relevant_do...\n",
      "\n",
      "üìÑ Source: qwen3_ollama | Distance: 0.6615\n",
      "üìù Preview: qwen3\n",
      "Models\n",
      "GitHub\n",
      "Discord\n",
      "Turbo\n",
      "Sign in\n",
      "Download\n",
      "Models\n",
      "Download\n",
      "GitHub\n",
      "Discord\n",
      "Sign in\n",
      "qwen3\n",
      "7.8M...\n",
      "\n",
      "üìÑ Source: sentence_transformers | Distance: 0.6685\n",
      "üìù Preview: start_multi_process_pool.\n",
      "Parameters\n",
      ":\n",
      "pool\n",
      "(\n",
      "Dict\n",
      "[\n",
      "str\n",
      ",\n",
      "object\n",
      "]\n",
      ") ‚Äì A dictionary containing the ...\n",
      "\n",
      "ü§ñ Generating response with qwen3:4b...\n",
      "\n",
      "<think>\n",
      "Let me analyze the question and the provided content to answer what the key features of Qwen3 model are.\n",
      "\n",
      "The question asks about the key features of Qwen3 model. Looking at the content provided, I can see that there's a section about Qwen3 in the text. Let me extract the relevant information.\n",
      "\n",
      "From the text:\n",
      "\n",
      "### Key features of Qwen3:\n",
      "\n",
      "1. **Significantly enhanced reasoning capabilities**: Surpasses previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n",
      "\n",
      "2. **Superior human preference alignment**: Excels in creative writing, role-playing, multi-turn dialogues, and instruction following, delivering a more natural, engaging, and immersive conversational experience.\n",
      "\n",
      "3. **Expertise in agent capabilities**: Enables precise integration with external tools in both thinking and unthinking modes, achieving leading performance among open-source models in complex agent-based tasks.\n",
      "\n",
      "4. **Support of 100+ languages and dialects**: With strong capabilities for multilingual instruction following and translation.\n",
      "\n",
      "The text also mentions some model specifications:\n",
      "- Qwen3 is a small MoE model (Mixture of Experts) that outcompetes QwQ-32B with 10 times of activated parameters\n",
      "- Even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct\n",
      "\n",
      "So the key features of Qwen3 model are:\n",
      "1. Enhanced reasoning capabilities\n",
      "2. Superior human preference alignment\n",
      "3. Expertise in agent capabilities\n",
      "4. Support for 100+ languages and dialects\n",
      "\n",
      "I'll formulate a concise answer based on these points.\n",
      "</think>\n",
      "\n",
      "# Key Features of Qwen3 Model\n",
      "\n",
      "Based on the provided information, Qwen3 has several notable key features:\n",
      "\n",
      "1. **Significantly enhanced reasoning capabilities** - Qwen3 surpasses previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n",
      "\n",
      "2. **Superior human preference alignment** - Excels in creative writing, role-playing, multi-turn dialogues, and instruction following, delivering a more natural, engaging, and immersive conversational experience.\n",
      "\n",
      "3. **Expertise in agent capabilities** - Enables precise integration with external tools in both thinking and unthinking modes, achieving leading performance among open-source models in complex agent-based tasks.\n",
      "\n",
      "4. **Multilingual support** - Supports 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.\n",
      "\n",
      "Additionally, Qwen3 demonstrates impressive efficiency with the small MoE (Mixture of Experts) architecture where even the Qwen3-4B model can rival the performance of larger Qwen2.5-72B-Instruct models.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Demo Question 3: About Qwen3\n",
    "response3 = rag_query(\"What are the key features of Qwen3 model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: How does vector similarity search work?\n",
      "============================================================\n",
      "üîç Found 3 relevant documents:\n",
      "üìÑ Source: huggingface_embeddinggemma_blog | Distance: 0.6704\n",
      "üìù Preview: prominent red spot.\"\n",
      ",\n",
      "\"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"\n",
      "]\n",
      "#...\n",
      "\n",
      "üìÑ Source: sentence_transformers | Distance: 0.6733\n",
      "üìù Preview: ,\n",
      "prompt_name\n",
      ":\n",
      "str\n",
      "|\n",
      "None\n",
      "=\n",
      "None\n",
      ",\n",
      "prompt\n",
      ":\n",
      "str\n",
      "|\n",
      "None\n",
      "=\n",
      "None\n",
      ",\n",
      "batch_size\n",
      ":\n",
      "int\n",
      "=\n",
      "32\n",
      ",\n",
      "show_progre...\n",
      "\n",
      "üìÑ Source: huggingface_embeddinggemma_blog | Distance: 0.6756\n",
      "üìù Preview: <bos>Welcome EmbeddingGemma, Google's new efficient embedding model\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spa...\n",
      "\n",
      "ü§ñ Generating response with qwen3:4b...\n",
      "\n",
      "<think>\n",
      "Let me analyze this problem carefully. The user has provided a lot of text, and I need to understand what they're asking for. \n",
      "\n",
      "First, I see that the user has provided:\n",
      "1. A description of embedding models (specifically EmbeddingGemma)\n",
      "2. Some code examples for using EmbeddingGemma with different frameworks\n",
      "3. A question at the end: \"How does vector similarity search work?\"\n",
      "\n",
      "The main question seems to be: \"How does vector similarity search work?\"\n",
      "\n",
      "Let me think about what vector similarity search is and how it works:\n",
      "\n",
      "Vector similarity search is a technique used in information retrieval and machine learning where we:\n",
      "1. Convert text (queries, documents) into vector representations (embeddings)\n",
      "2. Calculate the similarity between these vectors\n",
      "3. Find the most similar vectors to a given query vector\n",
      "\n",
      "The common similarity measures used are:\n",
      "- Cosine similarity\n",
      "- Euclidean distance\n",
      "- Dot product\n",
      "\n",
      "For vector similarity search, the typical workflow is:\n",
      "1. Preprocess text data (cleaning, tokenization)\n",
      "2. Convert text to embeddings (using a model like EmbeddingGemma)\n",
      "3. For a query, compute its embedding\n",
      "4. Find the closest embeddings in the database using a similarity metric\n",
      "5. Return the most relevant results\n",
      "\n",
      "In the context of the provided information, the user has shown examples with Sentence Transformers (a library for embedding models). The EmbeddingGemma model is specifically mentioned as a model that can be used for this purpose.\n",
      "\n",
      "The user might be asking for an explanation of how vector similarity search works, particularly in the context of EmbeddingGemma or the frameworks mentioned (Haystack, txtai, etc.).\n",
      "\n",
      "Let me formulate a comprehensive answer about vector similarity search:\n",
      "\n",
      "1. First, explain what vector similarity search is\n",
      "2. Explain the basic workflow\n",
      "3. Mention how EmbeddingGemma fits into this process\n",
      "4. Note the frameworks mentioned (Haystack, txtai, etc.) that implement this\n",
      "\n",
      "I'll write a clear, step-by-step explanation that addresses the question without assuming too much prior knowledge.\n",
      "</think>\n",
      "\n",
      "# How Vector Similarity Search Works\n",
      "\n",
      "Vector similarity search is a fundamental technique in modern information retrieval and natural language processing that finds relevant documents or results based on semantic similarity rather than exact keyword matching. Here's how it works:\n",
      "\n",
      "## Basic Workflow\n",
      "\n",
      "1. **Text to Vector Conversion**: \n",
      "   - Input text (queries or documents) is converted into dense numerical vectors using an embedding model\n",
      "   - In your case, models like EmbeddingGemma (Google's efficient multilingual embedding model) create these vectors\n",
      "\n",
      "2. **Similarity Calculation**:\n",
      "   - For a query vector, calculate similarity with all document vectors\n",
      "   - Most common metric: Cosine similarity (measures angle between vectors)\n",
      "   - Other metrics: Euclidean distance, dot product\n",
      "\n",
      "3. **Ranking and Retrieval**:\n",
      "   - Results are ranked by similarity score\n",
      "   - Top N most similar documents are returned as the answer\n",
      "\n",
      "## In Practice with EmbeddingGemma\n",
      "\n",
      "As shown in your examples, when using EmbeddingGemma with frameworks like Sentence Transformers:\n",
      "\n",
      "```python\n",
      "from sentence_transformers import SentenceTransformer\n",
      "\n",
      "# Load EmbeddingGemma model\n",
      "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
      "\n",
      "# Convert query and documents to embeddings\n",
      "query_embedding = model.encode(\"Which planet is known as the Red Planet?\")\n",
      "document_embeddings = model.encode([\n",
      "    \"Venus is often called Earth's twin because of its similar size and proximity.\",\n",
      "    \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\n",
      "    \"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\n",
      "    \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"\n",
      "])\n",
      "\n",
      "# Calculate similarity and find top results\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "\n",
      "# Calculate cosine similarity matrix\n",
      "similarity_matrix = cosine_similarity([query_embedding], [document_embeddings])[0]\n",
      "\n",
      "# Get top matching documents\n",
      "top_indices = np.argsort(similarity_matrix)[0][::-1][:3]\n",
      "top_results = [documents[i] for i in top_indices]\n",
      "```\n",
      "\n",
      "## Why This Matters\n",
      "\n",
      "Vector similarity search is powerful because:\n",
      "\n",
      "1. It understands semantic meaning rather than just keywords\n",
      "2. It works well with large datasets (millions of documents)\n",
      "3. It enables applications like:\n",
      "   - Semantic search\n",
      "   - Recommendation systems\n",
      "   - Chatbots with contextual understanding\n",
      "   - Document similarity detection\n",
      "\n",
      "The EmbeddingGemma model you mentioned has particular advantages for this task:\n",
      "- 308M parameters (small but powerful)\n",
      "- 2K context window (good for longer queries)\n",
      "- Supports over 100 languages\n",
      "- Truncatable output (768-dimensional vectors can be reduced to 512, 256, or 128 dimensions)\n",
      "\n",
      "In frameworks like Haystack and txtai, this process is implemented with optimized search algorithms (like ANN - Approximate Nearest Neighbors) to handle large-scale vector similarity search efficiently.\n",
      "\n",
      "This is why your example shows how EmbeddingGemma can find \"Mars, known for its reddish appearance, is often referred to as the Red Planet\" as the most relevant result for the query \"Which planet is known as the Red Planet?\"\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Demo Question 4: Technical concepts\n",
    "response4 = rag_query(\"How does vector similarity search work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully built a complete private RAG system! Here's what we accomplished:\n",
    "\n",
    "### ‚úÖ What We Built:\n",
    "- **Document Scraping**: Automated collection from web sources\n",
    "- **Smart Chunking**: Optimized text segmentation for better retrieval\n",
    "- **Modern Embeddings**: Google's EmbeddingGemma with Matryoshka learning\n",
    "- **Vector Database**: SQLite-vec for fast similarity search\n",
    "- **Local LLM**: Qwen3 for generating contextual responses\n",
    "- **Complete Privacy**: Everything runs locally, no API calls\n",
    "\n",
    "### üöÄ Key Benefits:\n",
    "- **100% Private**: All processing happens on your machine\n",
    "- **Zero Cost**: No API fees or usage limits\n",
    "- **Offline Capable**: Works without internet after initial setup\n",
    "- **Efficient**: EmbeddingGemma + SQLite-vec = fast performance\n",
    "- **Scalable**: Can handle thousands of documents\n",
    "\n",
    "### üîÑ Next Steps:\n",
    "- Add more document sources to expand knowledge base\n",
    "- Experiment with different chunking strategies\n",
    "- Try other embedding dimensions (128, 512, 768)\n",
    "- Implement conversation memory for multi-turn chats\n",
    "- Build a simple web interface with Streamlit or Gradio\n",
    "\n",
    "### üìö Resources:\n",
    "- [EmbeddingGemma Documentation](https://huggingface.co/google/embeddinggemma-300m)\n",
    "- [SQLite-vec GitHub](https://github.com/asg017/sqlite-vec)\n",
    "- [Ollama Models](https://ollama.com/library)\n",
    "\n",
    "Happy building! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup - close database connection\n",
    "conn.close()\n",
    "print(\"üßπ Database connection closed. Demo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EmbeddingGemma RAG",
   "language": "python",
   "name": "embeddinggemma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
